{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "object_detection_dir = 'C:/Users/User/0612_Merge_Json/Test_Image_Complete'\n",
    "classification_dir = 'C:/Users/User/0612_Merge_Json/Test_Image_Cut'\n",
    "webcrawler_dir = 'C:/Users/User/0612_Merge_Json/webcrawler'\n",
    "destination_dir = 'C:/Users/User/0612_Merge_Json/Merge_Json'\n",
    "ODList = os.listdir(object_detection_dir)\n",
    "CList = os.listdir(classification_dir)\n",
    "WList = os.listdir(webcrawler_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "strWList = []#存爬蟲的list\n",
    "strList = []#存物件偵測的list\n",
    "strCList = []#存分類器的list\n",
    "all_dict = {}#dictionary\n",
    "s = {}#dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\"'brand'\", \"'NET'\", \"'product_name'\", \"'女裝粉紅外套'\", \"'gender'\", \"'w'\", \"'price'\", \"'600'\", \"'website_url'\", \"'https\", \"//www.uniqlo.com/tw/store/goods/436738'\"]\n",
      "[\"'brand'\", \"'NET'\", \"'product_name'\", \"'女裝咖啡色長版外套'\", \"'gender'\", \"'w'\", \"'price'\", \"'900'\", \"'website_url'\", \"'https\", \"//www.uniqlo.com/tw/store/goods/436738'\"]\n",
      "[\"'brand'\", \"'NET'\", \"'product_name'\", \"'女裝米白百搭襯衫'\", \"'gender'\", \"'w'\", \"'price'\", \"'700'\", \"'website_url'\", \"'https\", \"//www.uniqlo.com/tw/store/goods/436738'\"]\n"
     ]
    }
   ],
   "source": [
    "for file in ODList:            \n",
    "    if file.endswith(\".json\"):\n",
    "        with open(os.path.join(object_detection_dir,file), 'r') as reader:\n",
    "            ODjf = json.loads(reader.read())\n",
    "        jf = str(ODjf).strip('}').strip('{')\n",
    "        newjf = str(jf).split(\":\")\n",
    "        # org_name為檔名，ex:NET_1.json\n",
    "        org_name = file.split(\".\")[0]\n",
    "        \n",
    "        #把原爬蟲的json檔{\"brand\":\"NET\",\"product_name\":\"女裝粉紅外套\",\"gender\":\"w\",\"price\":\"600\",\"website_url\":\"https://www.uniqlo.com/tw/store/goods/436738\"}\n",
    "        #拆為\"brand\",\"NET\",\"product_name\",\"女裝粉紅外套\",\"gender\",\"w\",\"price\",\"600\",\"website_url\",\"https://www.uniqlo.com/tw/store/goods/436738\"\n",
    "        #這邊有一個問題，用split(\":\")切割的話，網址也會被切掉，所以後面也會在合併\n",
    "        #存入strWList中，並且把前面有空格的地方都取刪掉\n",
    "        for file in WList:    \n",
    "            name = file.split(\".\")#ex:NET_1.json\n",
    "            if file.endswith(\".json\") and name[0] == org_name:\n",
    "                with open(os.path.join(webcrawler_dir,file), 'r',encoding = 'utf-8') as reader:#編碼要用utf-8才能存中文字\n",
    "                    Wjf = json.loads(reader.read())\n",
    "                jf = str(Wjf).strip('}').strip('{')\n",
    "                new1jf = str(jf).split(\":\")\n",
    "                \n",
    "                #new1jf[9]=new1jf[9]+new1jf[10]\n",
    "                for i in range(0,len(new1jf)):\n",
    "                    if ',' in str(new1jf[i]) :\n",
    "                        cut = str(new1jf[i]).split(\",\")                  \n",
    "                        strWList.append(cut[0].lstrip())\n",
    "                        strWList.append(cut[1].lstrip())\n",
    "                    else:\n",
    "                        strWList.append(new1jf[i].lstrip())\n",
    "        \n",
    "        #把原物件偵測的json檔{\"filename\":\"NET_1.jpg\",\"item1\":{\"Class\":\"long sleeve top\", \"Probability\": 0.7503372430801392, \"Bounding box\":[0.06489083, 0.28543484, 0.9990295, 0.96235776]},\"item2\":{\"Class\":\"trousers\", \"Probability\": 0.6078929901123047, \"Bounding box\":[0.1509296, 0.7262472, 0.738734, 1.0000496]}}\n",
    "        #拆為\"filename\",\"NET_1.jpg\",\"item1\",\"Class\",\"long sleeve top\",\"Probability\",0.7503372430801392,...,\"Bounding box\",0.1509296,0.7262472,0.738734,1.0000496\n",
    "        #存入strList中，並且把前面有空格的地方都取刪掉\n",
    "        for i in range(0,len(newjf)):\n",
    "            if ',' in str(newjf[i]) and '[' not in str(newjf[i]):\n",
    "                cut = str(newjf[i]).split(\",\")\n",
    "                strList.append(cut[0].lstrip())\n",
    "                strList.append(cut[1].lstrip())\n",
    "            elif '}' in str(newjf[i]):\n",
    "                cut = str(newjf[i]).split(\"}\")\n",
    "                strList.append(cut[0].lstrip())\n",
    "                cut_1 = cut[1].split(\",\")\n",
    "                strList.append(cut_1[1].lstrip())\n",
    "            elif '[' in str(newjf[i]) and '}' not in str(newjf[i]):    \n",
    "                strList.append(newjf[i].lstrip())\n",
    "            elif '{' in str(newjf[i]):\n",
    "                cut = str(newjf[i]).split(\"{\")\n",
    "                strList.append(cut[1].lstrip())\n",
    "            else:\n",
    "                strList.append(newjf[i].lstrip())  \n",
    "        \n",
    "        #把原分類器的json檔{\"Classification\":\"floral\",\"Highest_probability\":0.5591356158256531,\"Color\":\"red\"}\n",
    "        #拆為\"Classification\",\"floral\",\"Highest_probability\",0.5591356158256531,\"Color\",\"red\"\n",
    "        #存入strCList中，並且把前面有空格的地方都取刪掉\n",
    "        for file in CList:    \n",
    "            # name[0]+\"_\"+name[1]判斷是哪一張圖切下來的 、 item[0]判斷是哪一個 item\n",
    "            name = file.split(\"_\")#ex:NET_1_long sleeve top.json\n",
    "            name = name[0]+\"_\"+name[1] #ex:NET_1\n",
    "            cat = file.split(\"_\")\n",
    "            #print(name)\n",
    "            if file.endswith(\".json\") and name == org_name:\n",
    "                with open(os.path.join(classification_dir,file), 'r') as reader:\n",
    "                    Cjf = json.loads(reader.read())\n",
    "                jf = str(Cjf).strip('}').strip('{')\n",
    "                newjf = str(jf).split(\":\")  \n",
    "                item = cat[2].split(\".\")#判斷是哪個類別，ex:long sleeve top\n",
    "                strCList.append(item[0].lstrip())\n",
    "                for i in range(0,len(newjf)):\n",
    "                    if ',' in str(newjf[i]) :\n",
    "                        cut = str(newjf[i]).split(\",\")                  \n",
    "                        strCList.append(cut[0].lstrip())\n",
    "                        strCList.append(cut[1].lstrip())\n",
    "                    else:\n",
    "                        strCList.append(newjf[i].lstrip())\n",
    "        print(strWList)\n",
    "        #print(strList)\n",
    "        #print(strCList)\n",
    "        \n",
    "        #把\"filenam\"當成all_dict的key；對應的value是真正的檔名strList[1]，ex:\"NET_1.jpg\"\n",
    "        all_dict[strList[0]] = strList[1]  \n",
    "           \n",
    "        for j in range(0,1):\n",
    "            strWList[9] = strWList[9]+strWList[10]#把切錯的網址合併\n",
    "            all_dict[strWList[0]] = strWList[1]#把\"brand\"當成all_dict的key；對應的value為\"NET\"\n",
    "            all_dict[strWList[2]] = strWList[3]#把\"product_name\"當成all_dict的key；對應的value為\"女裝粉紅外套\"\n",
    "            all_dict[strWList[4]] = strWList[5]#把\"gender\"當成all_dict的key；對應的value為\"w\"\n",
    "            all_dict[strWList[6]] = strWList[7]#把\"price\"當成all_dict的key；對應的value為\"\"600\"\n",
    "            all_dict[strWList[8]] = strWList[9]#把\"website_url\"當成all_dict的key；對應的value為\"https://www.uniqlo.com/tw/store/goods/436738\"\n",
    "        \n",
    "        t = int( (len(strList) - 2)/7 )#有幾個item跑的次數\n",
    "        for i in range(0,t):\n",
    "            #把\"Class\",\"Probability\",\"Bounding box\"當s的key\n",
    "            #等號後是value:\"trousers\",0.6078929901123047,[0.1509296, 0.7262472, 0.738734, 1.0000496]\n",
    "            s[strList[3+7*i]] = strList[4+7*i]\n",
    "            s[strList[5+7*i]] = strList[6+7*i]\n",
    "            s[strList[7+7*i]] = strList[8+7*i]\n",
    "            # 判斷此分類器 json是哪一個 item的\n",
    "            for n in range(0,t):\n",
    "                if strCList[n*7] == s[\"'Class'\"].replace(\"'\",'').lstrip() :\n",
    "                    #把\"Classification\",\"Highest_probability\",\"Color\"當s的key\n",
    "                    #等號後是value:\"floral\",0.5591356158256531,\"red\"\n",
    "                    s[strCList[7*n+1]] = strCList[7*n+2]\n",
    "                    s[strCList[7*n+3]] = strCList[7*n+4]\n",
    "                    s[strCList[7*n+5]] = strCList[7*n+6]\n",
    "                    \n",
    "            #把\"item1\",\"item2\"當all_dict的key\n",
    "            #等號後是value:\"Class\",\"Probability\",\"Bounding box\",\"Classification\",\"Highest_probability\",\"Color\"\n",
    "            all_dict[strList[2+7*i]] = s\n",
    "            s = {}\n",
    "        \n",
    "        str_all_dict = str(all_dict).replace(\"'\",'')\n",
    "        all_dict = eval(str_all_dict)\n",
    "        filename = org_name + '.json'\n",
    "    \n",
    "        with open(os.path.join(destination_dir,filename), \"w\",encoding = 'utf-8') as f:\n",
    "            json.dump(all_dict, f,ensure_ascii=False) #不使用的ascii編碼，才不會讓中文字變亂碼\n",
    "            \n",
    "        strCList = []\n",
    "        strWList = []\n",
    "        strList = []\n",
    "        all_dict = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
